#!/usr/bin/env python3
"""
KKæ–‡æ¡£å¤„ç†å·¥å…· - ä¸»ç¨‹åºå…¥å£
å®Œæ•´ç‹¬ç«‹ç‰ˆæœ¬ï¼ŒåŒ…å«æ‰€æœ‰åŠŸèƒ½æ•´åˆ
æ”¯æŒï¼šæ‘˜è¦æå–ã€æ–‡æ¡£è½¬æ¢ã€ä¸­è‹±æ–‡æ–‡æ¡£å¯¹ç”Ÿæˆ
ä¸­è‹±æ–‡ç‰ˆæœ¬äº’ç›¸æŒ‡å‘å¯¹æ–¹é“¾æ¥
"""

import sys
import os
import argparse
import re
import signal
from pathlib import Path
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å°è¯•å¯¼å…¥document_processor
try:
    from document_processor import DocumentProcessor
    print("âœ… document_processor å¯¼å…¥æˆåŠŸ")
except ImportError:
    print("âš ï¸  document_processor å¯¼å…¥å¤±è´¥ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
    DocumentProcessor = None

# é»˜è®¤ç¤ºä¾‹æ–‡æœ¬
DEFAULT_TEXT = """è¿™æ˜¯ä¸€ä¸ªé»˜è®¤ç¤ºä¾‹æ–‡æ¡£ã€‚å½“æ²¡æœ‰æä¾›è¾“å…¥æ–‡æœ¬æ—¶ä½¿ç”¨æ­¤å†…å®¹ã€‚

## ç« èŠ‚æ ‡é¢˜
- é¡¹ç›®1ï¼šç¤ºä¾‹å†…å®¹
- é¡¹ç›®2ï¼šæ›´å¤šç¤ºä¾‹

ä¸»è¦æ®µè½å†…å®¹ä¼šåœ¨è¿™é‡Œå±•ç¤ºã€‚è¿™ä¸ªå·¥å…·ç”¨äºå¤„ç†æ–‡æ¡£è½¬æ¢å’Œæ‘˜è¦æå–ã€‚
"""

def read_file_content(filename):
    """è¯»å–æ–‡ä»¶å†…å®¹"""
    try:
        filepath = Path(filename)
        if not filepath.exists():
            return None
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            return content if content else None
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {filename}: {e}")
        return None

def get_input_text(args):
    """
    è·å–è¾“å…¥æ–‡æœ¬ï¼ŒæŒ‰ä¼˜å…ˆçº§å¤„ç†ï¼š
    1. --text å‚æ•°ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
    2. --file å‚æ•°
    3. text.txt æ–‡ä»¶ï¼ˆåå¤‡ï¼‰
    4. é»˜è®¤ç¤ºä¾‹æ–‡æœ¬ï¼ˆæœ€åï¼‰
    """
    # 1. å‘½ä»¤è¡Œç›´æ¥è¾“å…¥æ–‡æœ¬
    if args.text:
        print(f"ğŸ’¬ ä½¿ç”¨å‘½ä»¤è¡Œæ–‡æœ¬è¾“å…¥")
        return args.text
    
    # 2. æŒ‡å®šæ–‡ä»¶
    if args.file:
        content = read_file_content(args.file)
        if content:
            print(f"ğŸ“ ä»æŒ‡å®šæ–‡ä»¶è¯»å–: {args.file}")
            return content
        else:
            print(f"âš ï¸  æŒ‡å®šæ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©º: {args.file}")
    
    # 3. text.txt æ–‡ä»¶ï¼ˆåå¤‡ï¼‰
    content = read_file_content("text.txt")
    if content:
        print(f"ğŸ“ ä»text.txtè¯»å–")
        return content
    
    # 4. é»˜è®¤ç¤ºä¾‹æ–‡æœ¬
    print("âš ï¸  ä½¿ç”¨é»˜è®¤ç¤ºä¾‹æ–‡æœ¬")
    return DEFAULT_TEXT

def _split_paragraphs(content: str) -> list:
    """
    å°†å†…å®¹åˆ†å‰²æˆè‡ªç„¶æ®µæ•°ç»„
    åˆ†å‰²é€»è¾‘ï¼šé€šè¿‡è¿ç»­æ¢è¡Œï¼ˆç©ºè¡Œï¼‰æ¥åˆ†å‰²æ®µè½
    
    Args:
        content: åŸå§‹æ–‡æœ¬å†…å®¹
        
    Returns:
        list: è‡ªç„¶æ®µæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªæ®µè½ï¼ˆä¿ç•™å†…éƒ¨æ ¼å¼ï¼‰
    """
    paragraphs = []
    current_paragraph = []
    
    # æŒ‰è¡Œå¤„ç†ï¼Œä¿ç•™åŸæœ‰çš„æ¢è¡Œç»“æ„
    lines = content.split('\n')
    
    for line in lines:
        stripped_line = line.strip()
        
        if not stripped_line:  # ç©ºè¡Œï¼Œè¡¨ç¤ºæ®µè½åˆ†éš”
            if current_paragraph:  # å½“å‰æ®µè½æœ‰å†…å®¹
                paragraph_text = '\n'.join(current_paragraph)
                paragraphs.append(paragraph_text)
                current_paragraph = []
        else:
            # éç©ºè¡Œï¼Œæ·»åŠ åˆ°å½“å‰æ®µè½
            current_paragraph.append(line)
    
    # å¤„ç†æœ€åä¸€ä¸ªæ®µè½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if current_paragraph:
        paragraph_text = '\n'.join(current_paragraph)
        paragraphs.append(paragraph_text)
    
    # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ç©ºè¡Œåˆ†éš”ï¼Œä½†å†…å®¹éç©ºï¼Œåˆ™å°†æ•´ä¸ªå†…å®¹ä½œä¸ºä¸€ä¸ªæ®µè½
    if not paragraphs and content.strip():
        paragraphs = [content.strip()]
    
    return paragraphs

def _generate_heading(paragraph: str, max_words: int = 4) -> str:
    """
    ä¸ºæ®µè½ç”ŸæˆäºŒçº§æ ‡é¢˜ï¼Œè°ƒç”¨document_processorçš„summaryåŠŸèƒ½
    æ·»åŠ æ­»å¾ªç¯ä¿æŠ¤æœºåˆ¶å’Œè¶…æ—¶ä¿æŠ¤
    
    Args:
        paragraph: æ®µè½æ–‡æœ¬
        max_words: æ‘˜è¦æœ€å¤§å•è¯æ•°
        
    Returns:
        str: ç”Ÿæˆçš„æ ‡é¢˜æ–‡æœ¬
    """
    # é€’å½’ä¿æŠ¤ï¼šé˜²æ­¢å‡½æ•°è°ƒç”¨è‡ªèº«å¯¼è‡´æ­»å¾ªç¯
    if hasattr(_generate_heading, '_in_progress'):
        return "å†…å®¹æ‘˜è¦"
    
    # è®¾ç½®æ‰§è¡Œä¸­æ ‡å¿—
    _generate_heading._in_progress = True
    
    try:
        if DocumentProcessor is not None:
            # ç¡®ä¿DocumentProcessoråªåˆå§‹åŒ–ä¸€æ¬¡ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
            if not hasattr(_generate_heading, '_processor'):
                _generate_heading._processor = DocumentProcessor()
            
            processor = _generate_heading._processor
            
            # è¶…æ—¶å¤„ç†å‡½æ•°
            def timeout_handler(signum, frame):
                raise TimeoutError("æ ‡é¢˜ç”Ÿæˆè¶…æ—¶")
            
            # è®¾ç½®è¶…æ—¶ï¼ˆ10ç§’ï¼‰
            original_handler = signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(10)
            
            try:
                # è°ƒç”¨document_processorçš„æ‘˜è¦åŠŸèƒ½
                summary = processor.extract_summary(paragraph, max_words=max_words)
                signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
                
                if summary and summary.strip():
                    # æ¸…ç†æ ‡é¢˜æ ¼å¼
                    heading = summary.strip()
                    heading = re.sub(r'[.!?ã€‚ï¼ï¼Ÿ]+$', '', heading)
                    return heading
                    
            except TimeoutError:
                print("âš ï¸  æ ‡é¢˜ç”Ÿæˆè¶…æ—¶ï¼Œä½¿ç”¨é»˜è®¤æ ‡é¢˜")
                return "å†…å®¹æ‘˜è¦"
            except Exception as e:
                print(f"âŒ è°ƒç”¨document_processoræ‘˜è¦åŠŸèƒ½å¤±è´¥: {e}")
            finally:
                signal.alarm(0)  # ç¡®ä¿å–æ¶ˆè¶…æ—¶
                # æ¢å¤åŸæ¥çš„ä¿¡å·å¤„ç†å™¨
                signal.signal(signal.SIGALRM, original_handler)
                
    except Exception as e:
        print(f"âŒ æ ‡é¢˜ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    finally:
        # æ¸…é™¤æ‰§è¡Œä¸­æ ‡å¿—
        if hasattr(_generate_heading, '_in_progress'):
            delattr(_generate_heading, '_in_progress')
    
    # å¦‚æœç”Ÿæˆå¤±è´¥ï¼Œè¿”å›ç®€å•æ ‡é¢˜
    return "å†…å®¹æ‘˜è¦"

def _format_heading_markdown(heading_text: str) -> str:
    """
    å°†æ ‡é¢˜æ–‡æœ¬æ ¼å¼åŒ–ä¸ºMarkdownäºŒçº§æ ‡é¢˜æ ¼å¼
    
    Args:
        heading_text: æ ‡é¢˜æ–‡æœ¬
        
    Returns:
        str: Markdownæ ¼å¼çš„äºŒçº§æ ‡é¢˜ï¼Œå¦‚ "## æ ‡é¢˜å†…å®¹"
    """
    # æ¸…ç†æ ‡é¢˜æ–‡æœ¬
    cleaned_heading = heading_text.strip()
    
    # ç§»é™¤å¯èƒ½çš„å¤šä½™æ ‡ç‚¹
    cleaned_heading = re.sub(r'^[:\-\s]+|[:\-\s]+$', '', cleaned_heading)
    
    # ç¡®ä¿æ ‡é¢˜æ ¼å¼æ­£ç¡®
    if not cleaned_heading:
        cleaned_heading = "å†…å®¹æ‘˜è¦"
    
    # è¿”å›MarkdownäºŒçº§æ ‡é¢˜æ ¼å¼
    return f"## {cleaned_heading}"

def _process_content_with_headings(content: str, max_words: int = 4) -> str:
    """
    å¤„ç†å†…å®¹ï¼Œä¸ºè‡ªç„¶æ®µæ·»åŠ äºŒçº§æ ‡é¢˜
    
    Args:
        content: åŸå§‹æ–‡æœ¬å†…å®¹
        max_words: æ ‡é¢˜æ‘˜è¦å•è¯æ•°
        
    Returns:
        str: æ·»åŠ äº†äºŒçº§æ ‡é¢˜çš„æ–‡æœ¬å†…å®¹
    """
    print(f"ğŸ” å¼€å§‹å¤„ç†å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
    
    try:
        # ä½¿ç”¨_split_paragraphsåˆ†å‰²å†…å®¹
        paragraphs = _split_paragraphs(content)
        
        print(f"ğŸ“Š åˆ†å‰²å‡º {len(paragraphs)} ä¸ªè‡ªç„¶æ®µ")
        
        processed_paragraphs = []
        
        for i, paragraph in enumerate(paragraphs):
            print(f"ğŸ” æ­£åœ¨å¤„ç†æ®µè½ {i+1}/{len(paragraphs)}")
            
            try:
                cleaned_paragraph = paragraph.strip()
                if not cleaned_paragraph:
                    print(f"  æ®µè½ {i+1}: ç©ºæ®µè½ï¼Œè·³è¿‡")
                    continue
                
                print(f"  æ®µè½ {i+1} é•¿åº¦: {len(cleaned_paragraph)} å­—ç¬¦")
                    
                # ä¸ºé•¿æ®µè½ç”Ÿæˆæ ‡é¢˜
                if len(cleaned_paragraph) > 100:
                    print(f"  æ®µè½ {i+1}: éœ€è¦ç”Ÿæˆæ ‡é¢˜")
                    heading_text = _generate_heading(cleaned_paragraph, max_words)
                    markdown_heading = _format_heading_markdown(heading_text)
                    processed_paragraphs.append(f"{markdown_heading}\n\n{cleaned_paragraph}")
                    
                    print(f"  æ®µè½ {i+1}: æ·»åŠ æ ‡é¢˜ '{heading_text}'")
                else:
                    processed_paragraphs.append(cleaned_paragraph)
                    print(f"  æ®µè½ {i+1}: ä¿ç•™åŸæ ¼å¼")
                        
            except Exception as e:
                print(f"âŒ å¤„ç†æ®µè½ {i+1} æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                # å‡ºé”™æ—¶ä¿ç•™åŸæ®µè½
                processed_paragraphs.append(paragraph.strip())
        
        print(f"âœ… æ‰€æœ‰æ®µè½å¤„ç†å®Œæˆï¼Œå…± {len(processed_paragraphs)} ä¸ªæ®µè½")
        return '\n\n'.join(processed_paragraphs)
        
    except Exception as e:
        print(f"âŒ å¤„ç†å†…å®¹æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        # å‡ºé”™æ—¶è¿”å›åŸå§‹å†…å®¹
        return content

def _generate_markdown_pair(content: str, title: str, 
                           subject: str = "é€šç”¨",
                           original_url: str = "", 
                           publish_date: str = "",
                           output_dir: str = "."):
    """
    ç”Ÿæˆä¸­è‹±æ–‡Markdownæ–‡ä»¶å¯¹ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰
    ä¸­è‹±æ–‡ç‰ˆæœ¬äº’ç›¸æŒ‡å‘å¯¹æ–¹é“¾æ¥
    
    Args:
        content: åŸå§‹æ–‡æœ¬å†…å®¹
        title: æ–‡æ¡£æ ‡é¢˜
        subject: æ–‡æ¡£ä¸»é¢˜ï¼ˆå¦‚AIã€ç§‘æŠ€ç­‰ï¼‰
        original_url: åŸæ–‡URLï¼ˆå¯é€‰ï¼‰
        publish_date: å‘å¸ƒæ—¥æœŸï¼ˆå¯é€‰ï¼‰
        output_dir: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤å½“å‰ç›®å½•ï¼‰
        
    Returns:
        è‹±æ–‡ç‰ˆå’Œä¸­æ–‡ç‰ˆæ–‡ä»¶è·¯å¾„
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # é»˜è®¤å¤„ç†å†…å®¹ï¼Œæ·»åŠ äºŒçº§æ ‡é¢˜
    print("ğŸ“ æ­£åœ¨ä¸ºå†…å®¹æ·»åŠ äºŒçº§æ ‡é¢˜...")
    processed_content = _process_content_with_headings(content)
    
    # å¤„ç†å‘å¸ƒæ—¥æœŸ
    if not publish_date:
        publish_date = datetime.now().strftime("%Y-%m-%d")
    
    # ç”Ÿæˆæ–‡ä»¶å
    def _sanitize_filename(filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦"""
        return re.sub(r'[\\/*?:"<>|]', "", filename).replace(" ", "_")
    
    # ç”Ÿæˆæ–‡ä»¶å
    en_filename = f"{_sanitize_filename(title)}.md"
    cn_filename = f"{_sanitize_filename(title)}_cn.md"
    
    # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
    en_filepath = output_path / en_filename
    cn_filepath = output_path / cn_filename
    
    # ç”Ÿæˆäº’ç›¸æŒ‡å‘çš„URL
    en_url = f"https://github.com/Angelagoodboy/KK_Archive/blob/main/{en_filename}"
    cn_url = f"https://github.com/Angelagoodboy/KK_Archive/blob/main/{cn_filename}"
    
    # ç”Ÿæˆè‹±æ–‡ç‰ˆï¼ˆæŒ‡å‘ä¸­æ–‡ç‰ˆï¼‰
    en_metadata = f"""> **Publish Date**: {publish_date}  
> **Author**: Kevin Kelly  
> **Subject**: {subject}  

**Document Information**
- Original URL: {original_url if original_url else "Not provided"}
- Chinese Version: [{cn_url}]({cn_url})"""
    
    en_md = f"""# {title}

{en_metadata}

{processed_content}

---

*Document generated by KK Document Processor*
"""
    
    # ç”Ÿæˆä¸­æ–‡ç‰ˆï¼ˆæŒ‡å‘è‹±æ–‡ç‰ˆï¼‰
    cn_title = f"{title}ï¼ˆä¸­æ–‡ç‰ˆï¼‰"
    cn_metadata = f"""> **å‘å¸ƒæ—¶é—´**: {publish_date}  
> **ä½œè€…**: å‡¯æ–‡Â·å‡¯åˆ© (Kevin Kelly)  
> **ä¸»é¢˜**: {subject}  

**æ–‡æ¡£ä¿¡æ¯**
- åŸæ–‡åœ°å€: {original_url if original_url else "æœªæä¾›"}
- è‹±æ–‡ç‰ˆæœ¬: [{en_url}]({en_url})"""
    
    cn_md = f"""# {cn_title}

{cn_metadata}

{processed_content}

---

*æ–‡æ¡£ç”± KK æ–‡æ¡£å¤„ç†å™¨ç”Ÿæˆ*
"""
    
    # å†™å…¥æ–‡ä»¶
    with open(en_filepath, 'w', encoding='utf-8') as f:
        f.write(en_md)
    
    with open(cn_filepath, 'w', encoding='utf-8') as f:
        f.write(cn_md)
    
    print(f"âœ… æ–‡ä»¶ç”Ÿæˆå®Œæˆ:")
    print(f"   â€¢ è‹±æ–‡ç‰ˆ: {en_filepath}")
    print(f"   â€¢ ä¸­æ–‡ç‰ˆ: {cn_filepath}")
    
    return str(en_filepath), str(cn_filepath)

def extract_summary(args):
    """æå–æ–‡æœ¬æ‘˜è¦"""
    if DocumentProcessor is None:
        print("âŒ document_processor ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œæ‘˜è¦æå–")
        return
    
    text = get_input_text(args)
    
    processor = DocumentProcessor()
    
    print(f"ğŸ“ è¾“å…¥æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
    if args.verbose:
        preview = text[:200] + "..." if len(text) > 200 else text
        print(f"ğŸ“‹ å†…å®¹é¢„è§ˆ: {preview}")
    
    print("ğŸ¤– æ­£åœ¨æå–æ‘˜è¦...")
    try:
        summary = processor.extract_summary(text, args.max_words)
        word_count = len(summary.split())
        
        print(f"âœ… æ‘˜è¦æå–å®Œæˆ!")
        print(f"ğŸ“Š æ‘˜è¦ ({word_count}/{args.max_words} å•è¯):")
        print(f"   {summary}")
    except Exception as e:
        print(f"âŒ æ‘˜è¦æå–å¤±è´¥: {e}")
        # å¤‡ç”¨æ–¹æ¡ˆï¼šæå–å‰Nä¸ªå•è¯
        words = text.split()[:args.max_words]
        fallback_summary = " ".join(words)
        print(f"ğŸ”„ ä½¿ç”¨å¤‡ç”¨æ‘˜è¦: {fallback_summary}")

def convert_to_markdown(args):
    """è½¬æ¢æ–‡æ¡£ä¸ºç»“æ„åŒ–Markdown"""
    if DocumentProcessor is None:
        print("âŒ document_processor ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œæ–‡æ¡£è½¬æ¢")
        return
    
    text = get_input_text(args)
    
    # å¤„ç†å†…å®¹ï¼Œæ·»åŠ äºŒçº§æ ‡é¢˜
    print("ğŸ“ æ­£åœ¨ä¸ºå†…å®¹æ·»åŠ äºŒçº§æ ‡é¢˜...")
    processed_text = _process_content_with_headings(text)
    
    processor = DocumentProcessor()
    print(f"ğŸ“„ æ­£åœ¨å¤„ç†æ–‡æ¡£: {args.title}")
    
    try:
        structured_md = processor.convert_to_structured_md(
            processed_text, args.title, args.url, args.date
        )
        
        # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
        if args.output:
            output_dir = Path(args.output)
            output_dir.mkdir(exist_ok=True)
            output_file = output_dir / f"{processor.generate_cn_filename(args.title)}.md"
        else:
            output_file = Path(f"{processor.generate_cn_filename(args.title)}.md")
        
        # å†™å…¥æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(structured_md)
        
        print(f"âœ… è½¬æ¢å®Œæˆ!")
        print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"ğŸ“Š æ–‡æ¡£å¤§å°: {len(structured_md)} å­—ç¬¦")
        
        if args.verbose:
            print(f"ğŸ“„ å†…å®¹é¢„è§ˆ:")
            print(structured_md[:300] + "..." if len(structured_md) > 300 else structured_md)
        
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")

def generate_markdown_pair(args):
    """ç”Ÿæˆä¸­è‹±æ–‡Markdownæ–‡ä»¶å¯¹"""
    text = get_input_text(args)
    
    print(f"ğŸ“„ æ­£åœ¨ç”Ÿæˆæ–‡æ¡£å¯¹: {args.title}")
    print(f"ğŸ·ï¸ æ–‡æ¡£ä¸»é¢˜: {args.subject}")
    if args.output:
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output}")
    
    try:
        # ä½¿ç”¨å†…éƒ¨å‡½æ•°ç”Ÿæˆæ–‡æ¡£å¯¹
        en_file, cn_file = _generate_markdown_pair(
            content=text,
            title=args.title,
            subject=args.subject,
            original_url=args.url,
            publish_date=args.date,
            output_dir=args.output
        )
        
        print(f"âœ… ç”Ÿæˆå®Œæˆ!")
        print(f"   â€¢ è‹±æ–‡ç‰ˆ: {en_file}")
        print(f"   â€¢ ä¸­æ–‡ç‰ˆ: {cn_file}")
        
    except Exception as e:
        print(f"âŒ æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {e}")

def setup_parser():
    """è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description="KKæ–‡æ¡£å¤„ç†å·¥å…· - æ™ºèƒ½æ–‡æ¡£è½¬æ¢å’Œæ‘˜è¦æå–",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  æ‘˜è¦æå–: python main.py summary --text "è¦æ‘˜è¦çš„æ–‡æœ¬"
  æ–‡æ¡£è½¬æ¢: python main.py convert --title "æ–‡æ¡£æ ‡é¢˜" --text "å†…å®¹"
  ç”Ÿæˆæ–‡æ¡£å¯¹: python main.py generate --title "æ ‡é¢˜" --subject "AI" --file input.txt --output docs/
        """
    )
    
    # å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='é€‰æ‹©æ“ä½œå‘½ä»¤')
    
    # é€šç”¨å‚æ•°ç»„ï¼ˆæ‰€æœ‰å‘½ä»¤å…±äº«ï¼‰
    input_group = argparse.ArgumentParser(add_help=False)
    input_group.add_argument('--text', '-t', help='ç›´æ¥è¾“å…¥æ–‡æœ¬å†…å®¹')
    input_group.add_argument('--file', '-f', help='ä»æ–‡ä»¶è¯»å–å†…å®¹')
    input_group.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡ºæ¨¡å¼')
    input_group.add_argument('--quiet', '-q', action='store_true', help='é™é»˜æ¨¡å¼')
    
    # summary å‘½ä»¤
    summary_parser = subparsers.add_parser('summary', help='æå–æ–‡æœ¬æ‘˜è¦', parents=[input_group])
    summary_parser.add_argument('--max-words', '-w', type=int, default=5, help='æ‘˜è¦æœ€å¤§å•è¯æ•°')
    
    # convert å‘½ä»¤
    convert_parser = subparsers.add_parser('convert', help='è½¬æ¢ä¸ºMarkdownæ–‡æ¡£', parents=[input_group])
    convert_parser.add_argument('--title', required=True, help='æ–‡æ¡£æ ‡é¢˜')
    convert_parser.add_argument('--url', '-u', default='', help='åŸæ–‡URL')
    convert_parser.add_argument('--date', '-d', default='', help='å‘å¸ƒæ—¥æœŸ')
    convert_parser.add_argument('--output', '-o', help='è¾“å‡ºç›®å½•')
    
    # generate å‘½ä»¤
    generate_parser = subparsers.add_parser('generate', help='ç”Ÿæˆä¸­è‹±æ–‡æ–‡æ¡£å¯¹', parents=[input_group])
    generate_parser.add_argument('--title', required=True, help='æ–‡æ¡£æ ‡é¢˜')
    generate_parser.add_argument('--subject', '-s', default='é€šç”¨', help='æ–‡æ¡£ä¸»é¢˜ï¼ˆå¦‚AIã€ç§‘æŠ€ç­‰ï¼‰')
    generate_parser.add_argument('--url', '-u', default='', help='åŸæ–‡URL')
    generate_parser.add_argument('--date', '-d', default='', help='å‘å¸ƒæ—¥æœŸ')
    generate_parser.add_argument('--output', '-o', default='.', help='è¾“å‡ºç›®å½•')
    
    return parser

def main():
    """ä¸»å‡½æ•°"""
    parser = setup_parser()
    
    if len(sys.argv) == 1:
        parser.print_help()
        return
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    try:
        if args.command == 'summary':
            extract_summary(args)
        elif args.command == 'convert':
            convert_to_markdown(args)
        elif args.command == 'generate':
            generate_markdown_pair(args)
        else:
            parser.print_help()
            
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()